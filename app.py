# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/113oRLaFNWD6QtQ6eGrxIhEFkmlvueZT9
"""

import pandas as pd
import numpy as np
import streamlit as st
import plotly.express as px
import plotly.graph_objects as go

# ----------------------------
# Page setup
# ----------------------------
st.set_page_config(page_title="Approval Insights Dashboard", layout="wide")
st.title("Approval Insights Dashboard")
st.caption(
    "Interactive dashboard for approval behavior, fast approvals, block analysis, and hypothesis tests. "
    "Includes filters, KPI trends, benchmark lines, risk scoring, heatmaps, and downloads."
)

# ----------------------------
# Helpers
# ----------------------------
@st.cache_data
def load_excel(uploaded_file):
    xls = pd.ExcelFile(uploaded_file)
    return {name: pd.read_excel(xls, name) for name in xls.sheet_names}

def ensure_datetime(df, col):
    if df is not None and col in df.columns:
        df[col] = pd.to_datetime(df[col], errors="coerce")
    return df

def safe_col(df, candidates):
    if df is None:
        return None
    for c in candidates:
        if c in df.columns:
            return c
    return None

def to_num(s):
    return pd.to_numeric(s, errors="coerce")

def add_duration_benchmarks(fig, axis="x", unit="sec"):
    """
    Add benchmark reference lines for duration:
      10 min, 5 min, 2 min.
    For histograms where duration is on x-axis -> axis="x" (vertical lines).
    For bar charts where duration is on y-axis -> axis="y" (horizontal lines).
    """
    if unit == "sec":
        points = [(600, "10 min"), (300, "5 min"), (120, "2 min")]
    else:
        points = [(10, "10 min"), (5, "5 min"), (2, "2 min")]

    for v, label in points:
        if axis == "x":
            fig.add_vline(x=v, line_dash="dash", annotation_text=label, annotation_position="top")
        else:
            fig.add_hline(y=v, line_dash="dash", annotation_text=label, annotation_position="top left")
    return fig

def download_button_df(df: pd.DataFrame, label: str, filename: str, key: str):
    csv = df.to_csv(index=False).encode("utf-8")
    st.download_button(label=label, data=csv, file_name=filename, mime="text/csv", key=key)

# ----------------------------
# Upload
# ----------------------------
uploaded = st.file_uploader("Upload approval_analysis_output.xlsx", type=["xlsx"])
if not uploaded:
    st.info("ðŸ‘† Upload `approval_analysis_output.xlsx` to load the dashboard.")
    st.stop()

sheets = load_excel(uploaded)
st.success(f"Loaded sheets: {', '.join(sheets.keys())}")

# Known tabs (from your screenshot)
clean = sheets.get("clean_data")
tech = sheets.get("technician_summary")
block_kpis = sheets.get("block_kpis")
one_sample = sheets.get("one_sample_ttests")
payout = sheets.get("payout_two_sample")
within_tech = sheets.get("within_block_tech_summary")
within_dist = sheets.get("within_block_distributions")
block_first_rest = sheets.get("block_first_vs_rest_summary")
block_tests = sheets.get("block_first_vs_rest_tests")
sus_blocks = sheets.get("suspicious_blocks")

# Clean data required
if clean is None:
    st.error("Sheet `clean_data` not found. Your Excel must include a `clean_data` tab.")
    st.stop()

# Ensure datetimes
clean = ensure_datetime(clean, "approval_dt")
clean = ensure_datetime(clean, "prev_approval_dt")

# Identify key columns in clean_data
tech_col = safe_col(clean, ["technician", "Technician", "provider_approving_name"])
dt_col = safe_col(clean, ["approval_dt", "Approval_DT", "approval_date"])
weekday_col = safe_col(clean, ["weekday", "Weekday"])
hour_col = safe_col(clean, ["hour", "Hour"])
duration_sec_col = safe_col(clean, ["duration_sec", "duration_seconds", "Duration_Sec"])
same_second_col = safe_col(clean, ["same_second_as_prev", "same_second"])
same_minute_col = safe_col(clean, ["same_minute_as_prev", "same_minute"])

if tech_col is None or dt_col is None:
    st.error("`clean_data` must contain `technician` and `approval_dt` columns.")
    st.stop()

df = clean.copy()

# ----------------------------
# Sidebar Filters
# ----------------------------
st.sidebar.header("Filters")

dt_series = pd.to_datetime(df[dt_col], errors="coerce")
min_date = pd.to_datetime(dt_series.min()).date()
max_date = pd.to_datetime(dt_series.max()).date()

date_range = st.sidebar.date_input(
    "Date range",
    value=(min_date, max_date),
    min_value=min_date,
    max_value=max_date
)

if isinstance(date_range, tuple) and len(date_range) == 2:
    start_date, end_date = date_range
else:
    start_date, end_date = min_date, max_date

all_techs = sorted(df[tech_col].dropna().astype(str).unique())
selected_techs = st.sidebar.multiselect("Technician(s)", all_techs, default=all_techs)

if weekday_col:
    weekdays = sorted([w for w in df[weekday_col].dropna().unique()])
    selected_weekdays = st.sidebar.multiselect("Weekday(s)", weekdays, default=weekdays)
else:
    selected_weekdays = None

if hour_col:
    h = to_num(df[hour_col])
    min_h = int(np.nanmin(h))
    max_h = int(np.nanmax(h))
    hour_range = st.sidebar.slider("Hour range", min_h, max_h, (min_h, max_h))
else:
    hour_range = None

mask = (
    df[tech_col].astype(str).isin([str(x) for x in selected_techs]) &
    (pd.to_datetime(df[dt_col]).dt.date >= start_date) &
    (pd.to_datetime(df[dt_col]).dt.date <= end_date)
)

if weekday_col and selected_weekdays is not None:
    mask &= df[weekday_col].isin(selected_weekdays)

if hour_col and hour_range is not None:
    hh = to_num(df[hour_col])
    mask &= (hh >= hour_range[0]) & (hh <= hour_range[1])

fdf = df.loc[mask].copy()

# ----------------------------
# KPI Row + Trend Deltas (previous equal-length period)
# ----------------------------
total_days = (pd.to_datetime(end_date) - pd.to_datetime(start_date)).days + 1
prev_end = pd.to_datetime(start_date) - pd.Timedelta(days=1)
prev_start = prev_end - pd.Timedelta(days=total_days - 1)

pmask = (
    df[tech_col].astype(str).isin([str(x) for x in selected_techs]) &
    (pd.to_datetime(df[dt_col]).dt.date >= prev_start.date()) &
    (pd.to_datetime(df[dt_col]).dt.date <= prev_end.date())
)

if weekday_col and selected_weekdays is not None:
    pmask &= df[weekday_col].isin(selected_weekdays)

if hour_col and hour_range is not None:
    hh = to_num(df[hour_col])
    pmask &= (hh >= hour_range[0]) & (hh <= hour_range[1])

pdf = df.loc[pmask].copy()

def kpi_stats(dd):
    out = {}
    out["approvals"] = len(dd)
    out["techs"] = dd[tech_col].nunique()
    if duration_sec_col:
        dur = to_num(dd[duration_sec_col])
        out["median_sec"] = float(np.nanmedian(dur))
        out["mean_sec"] = float(np.nanmean(dur))
        out["fast_lt_5_pct"] = float(np.nanmean(dur < 5) * 100)
        out["fast_lt_2_pct"] = float(np.nanmean(dur < 2) * 100)
    else:
        out["median_sec"] = np.nan
        out["mean_sec"] = np.nan
        out["fast_lt_5_pct"] = np.nan
        out["fast_lt_2_pct"] = np.nan
    return out

cur = kpi_stats(fdf)
prev = kpi_stats(pdf)

k1, k2, k3, k4, k5 = st.columns(5)
k1.metric("Total approvals", f"{cur['approvals']:,}", delta=f"{cur['approvals'] - prev['approvals']:+,}")
k2.metric("Technicians", f"{cur['techs']:,}", delta=f"{cur['techs'] - prev['techs']:+,}")
k3.metric(
    "Median duration (sec)",
    f"{cur['median_sec']:.2f}" if np.isfinite(cur["median_sec"]) else "NA",
    delta=f"{(cur['median_sec'] - prev['median_sec']):+.2f}" if np.isfinite(prev["median_sec"]) and np.isfinite(cur["median_sec"]) else None
)
k4.metric(
    "Mean duration (sec)",
    f"{cur['mean_sec']:.2f}" if np.isfinite(cur["mean_sec"]) else "NA",
    delta=f"{(cur['mean_sec'] - prev['mean_sec']):+.2f}" if np.isfinite(prev["mean_sec"]) and np.isfinite(cur["mean_sec"]) else None
)
k5.metric(
    "Fast < 5 sec (%)",
    f"{cur['fast_lt_5_pct']:.1f}%" if np.isfinite(cur["fast_lt_5_pct"]) else "NA",
    delta=f"{(cur['fast_lt_5_pct'] - prev['fast_lt_5_pct']):+.1f}%" if np.isfinite(prev["fast_lt_5_pct"]) and np.isfinite(cur["fast_lt_5_pct"]) else None
)

st.divider()

# ----------------------------
# Risk Score (composite)
# ----------------------------
st.subheader("Technician Risk Scoring (Composite)")

with st.expander("How the Risk Score is computed (simple + explainable)"):
    st.write(
        "- **Fast < 2 sec %** (weight 0.45)\n"
        "- **Same-second rate** (weight 0.30)\n"
        "- **Same-minute rate** (weight 0.15)\n"
        "- **Suspicious blocks count** (weight 0.10)\n\n"
        "Score is normalized to 0â€“100 within the filtered data."
    )

risk_df = fdf.groupby(tech_col).agg(approvals=(tech_col, "size")).reset_index()

# fast <2%
if duration_sec_col:
    tmp = fdf.assign(_dur=to_num(fdf[duration_sec_col]))
    r_fast2 = tmp.groupby(tech_col)["_dur"].apply(lambda x: float(np.nanmean(x < 2) * 100)).reset_index(name="fast_lt_2_pct")
else:
    r_fast2 = pd.DataFrame({tech_col: risk_df[tech_col], "fast_lt_2_pct": np.nan})

# same-second / same-minute
if same_second_col:
    r_ss = fdf.groupby(tech_col)[same_second_col].mean().reset_index(name="same_second_rate")
    r_ss["same_second_rate"] *= 100
else:
    r_ss = pd.DataFrame({tech_col: risk_df[tech_col], "same_second_rate": np.nan})

if same_minute_col:
    r_sm = fdf.groupby(tech_col)[same_minute_col].mean().reset_index(name="same_minute_rate")
    r_sm["same_minute_rate"] *= 100
else:
    r_sm = pd.DataFrame({tech_col: risk_df[tech_col], "same_minute_rate": np.nan})

# suspicious blocks count
if sus_blocks is not None:
    s_tech_col = safe_col(sus_blocks, ["technician", "Technician"])
    if s_tech_col:
        sb = sus_blocks.groupby(s_tech_col).size().reset_index(name="suspicious_blocks").rename(columns={s_tech_col: tech_col})
    else:
        sb = pd.DataFrame({tech_col: risk_df[tech_col], "suspicious_blocks": 0})
else:
    sb = pd.DataFrame({tech_col: risk_df[tech_col], "suspicious_blocks": 0})

risk = risk_df.merge(r_fast2, on=tech_col, how="left") \
              .merge(r_ss, on=tech_col, how="left") \
              .merge(r_sm, on=tech_col, how="left") \
              .merge(sb, on=tech_col, how="left")

def norm01(series):
    s = to_num(series)
    if np.nanmax(s) == np.nanmin(s):
        return pd.Series(np.zeros(len(s)))
    return (s - np.nanmin(s)) / (np.nanmax(s) - np.nanmin(s))

w_fast2, w_ss, w_sm, w_sb = 0.45, 0.30, 0.15, 0.10
risk["risk_score_0_100"] = 100 * (
    w_fast2 * norm01(risk["fast_lt_2_pct"]) +
    w_ss * norm01(risk["same_second_rate"]) +
    w_sm * norm01(risk["same_minute_rate"]) +
    w_sb * norm01(risk["suspicious_blocks"])
)

risk = risk.sort_values("risk_score_0_100", ascending=False)

cA, cB = st.columns([1.2, 1])
with cA:
    st.dataframe(
        risk[[tech_col, "approvals", "fast_lt_2_pct", "same_second_rate", "same_minute_rate", "suspicious_blocks", "risk_score_0_100"]].head(50),
        use_container_width=True
    )
with cB:
    fig = px.bar(risk.head(20), x=tech_col, y="risk_score_0_100", title="")
    st.plotly_chart(fig, use_container_width=True)

download_button_df(
    risk[[tech_col, "approvals", "fast_lt_2_pct", "same_second_rate", "same_minute_rate", "suspicious_blocks", "risk_score_0_100"]],
    "Download Risk Scores (CSV)",
    "risk_scores.csv",
    "dl_risk"
)

st.divider()

# ----------------------------
# Tabs
# ----------------------------
tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
    "Overview",
    "Technician Deep Dive",
    "Heatmap",
    "Block Analysis",
    "Hypothesis Tests",
    "Suspicious Blocks"
])

# ----------------------------
# TAB 1: Overview
# ----------------------------
with tab1:
    left, right = st.columns([1.2, 1])

    with left:
        st.subheader("Approvals over time")
        tmp = fdf.copy()
        tmp["date"] = pd.to_datetime(tmp[dt_col]).dt.date
        daily = tmp.groupby("date").size().reset_index(name="approvals")
        fig = px.line(daily, x="date", y="approvals", markers=True)
        st.plotly_chart(fig, use_container_width=True)

    with right:
        st.subheader("Duration distribution (with benchmark lines)")
        if duration_sec_col:
            tmp = fdf.copy()
            tmp["_dur"] = to_num(tmp[duration_sec_col])
            tmp = tmp.dropna(subset=["_dur"])
            fig = px.histogram(tmp, x="_dur", nbins=60)
            # FIX: duration is on x-axis, so use vertical lines
            fig = add_duration_benchmarks(fig, axis="x", unit="sec")
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("Duration column not found in clean_data.")

    st.subheader("Top technicians by approval volume")
    top = fdf.groupby(tech_col).size().reset_index(name="approvals").sort_values("approvals", ascending=False).head(25)
    fig = px.bar(top, x=tech_col, y="approvals")
    st.plotly_chart(fig, use_container_width=True)

    download_button_df(fdf, "Download Filtered clean_data (CSV)", "clean_data_filtered.csv", "dl_clean_filtered")

# ----------------------------
# TAB 2: Technician Deep Dive
# ----------------------------
with tab2:
    st.subheader("Technician summary (from technician_summary sheet)")
    if tech is not None:
        st.dataframe(tech, use_container_width=True)

        tcol = safe_col(tech, ["technician", "Technician"])
        approvals_col = safe_col(tech, ["n_approvals", "approvals", "count"])
        med_col = safe_col(tech, ["median_duration_sec", "median_sec", "median_duration"])
        fast5_col = safe_col(tech, ["pct_fast_lt_5s", "fast_lt_5s_pct", "pct_fast_5s"])

        c1, c2 = st.columns(2)
        with c1:
            if tcol and approvals_col:
                fig = px.bar(
                    tech.sort_values(approvals_col, ascending=False).head(25),
                    x=tcol, y=approvals_col,
                    title="Approvals by technician"
                )
                st.plotly_chart(fig, use_container_width=True)

        with c2:
            if tcol and med_col:
                fig = px.bar(
                    tech.sort_values(med_col, ascending=False).head(25),
                    x=tcol, y=med_col,
                    title="Median duration (sec) by technician"
                )
                # Here duration is on y-axis -> horizontal lines
                fig = add_duration_benchmarks(fig, axis="y", unit="sec")
                st.plotly_chart(fig, use_container_width=True)

        if tcol and fast5_col:
            fig = px.bar(
                tech.sort_values(fast5_col, ascending=False).head(25),
                x=tcol, y=fast5_col,
                title="Fast < 5 sec (%) by technician"
            )
            st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("Sheet `technician_summary` not found.")

    st.subheader("Filtered raw approvals (preview)")
    st.dataframe(fdf.head(500), use_container_width=True)

# ----------------------------
# TAB 3: Heatmap (Weekday x Hour)
# ----------------------------
with tab3:
    st.subheader("Heatmap: Weekday Ã— Hour (Median duration in seconds)")
    if weekday_col and hour_col and duration_sec_col:
        tmp = fdf.copy()
        tmp["_hour"] = to_num(tmp[hour_col]).astype("Int64")
        tmp["_dur"] = to_num(tmp[duration_sec_col])
        tmp = tmp.dropna(subset=["_hour", "_dur", weekday_col])

        pivot = tmp.pivot_table(
            index=weekday_col,
            columns="_hour",
            values="_dur",
            aggfunc="median"
        )

        fig = go.Figure(
            data=go.Heatmap(
                z=pivot.values,
                x=[int(x) for x in pivot.columns],
                y=pivot.index.astype(str).tolist(),
                colorbar=dict(title="Median sec")
            )
        )
        fig.update_layout(xaxis_title="Hour of day", yaxis_title="Weekday", height=450)
        st.plotly_chart(fig, use_container_width=True)
        st.caption("Use filters to focus on technicians, dates, weekdays, or hours.")
    else:
        st.info("Heatmap needs `weekday`, `hour`, and `duration_sec` columns in `clean_data`.")

# ----------------------------
# TAB 4: Block Analysis
# ----------------------------
with tab4:
    st.subheader("Block KPIs")
    if block_kpis is not None:
        st.dataframe(block_kpis, use_container_width=True)
        download_button_df(block_kpis, "Download Block KPIs (CSV)", "block_kpis.csv", "dl_block_kpis")
    else:
        st.info("Sheet `block_kpis` not found.")

    st.subheader("Within-block distributions")
    if within_dist is not None:
        st.dataframe(within_dist, use_container_width=True)
        download_button_df(within_dist, "Download Within-block Distributions (CSV)", "within_block_distributions.csv", "dl_within_dist")
    else:
        st.info("Sheet `within_block_distributions` not found.")

    st.subheader("Within-block technician summary")
    if within_tech is not None:
        st.dataframe(within_tech, use_container_width=True)
        download_button_df(within_tech, "Download Within-block Technician Summary (CSV)", "within_block_tech_summary.csv", "dl_within_tech")
    else:
        st.info("Sheet `within_block_tech_summary` not found.")

    st.subheader("First approval vs Rest of block")
    if block_first_rest is not None:
        st.dataframe(block_first_rest, use_container_width=True)

        btech = safe_col(block_first_rest, ["technician", "Technician"])
        first_col = safe_col(block_first_rest, ["first_mean_sec", "first_avg_sec", "first_mean", "first_duration_mean_sec", "first_median_sec"])
        rest_col  = safe_col(block_first_rest, ["rest_mean_sec", "rest_avg_sec", "rest_mean", "rest_duration_mean_sec", "rest_median_sec"])

        if btech and first_col and rest_col:
            plot_df = block_first_rest[[btech, first_col, rest_col]].dropna().head(25)
            melted = plot_df.melt(id_vars=[btech], value_vars=[first_col, rest_col],
                                  var_name="block_part", value_name="duration_sec")
            fig = px.bar(melted, x=btech, y="duration_sec", color="block_part", barmode="group",
                         title="First approval vs Rest")
            fig = add_duration_benchmarks(fig, axis="y", unit="sec")
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("Could not auto-detect first/rest duration columns in block_first_vs_rest_summary.")

        download_button_df(block_first_rest, "Download First vs Rest Summary (CSV)", "block_first_vs_rest_summary.csv", "dl_first_rest")
    else:
        st.info("Sheet `block_first_vs_rest_summary` not found.")

# ----------------------------
# TAB 5: Hypothesis Tests
# ----------------------------
with tab5:
    st.subheader("One-sample t-tests vs benchmarks")
    if one_sample is not None:
        st.dataframe(one_sample, use_container_width=True)
        download_button_df(one_sample, "Download One-sample T-tests (CSV)", "one_sample_ttests.csv", "dl_one_sample")
    else:
        st.info("Sheet `one_sample_ttests` not found.")

    st.subheader("Payout two-sample test")
    if payout is not None:
        st.dataframe(payout, use_container_width=True)
        download_button_df(payout, "Download Payout Two-sample Results (CSV)", "payout_two_sample.csv", "dl_payout")
    else:
        st.info("Sheet `payout_two_sample` not found.")

    st.subheader("Block first vs rest tests")
    if block_tests is not None:
        st.dataframe(block_tests, use_container_width=True)
        download_button_df(block_tests, "Download Block First vs Rest Tests (CSV)", "block_first_vs_rest_tests.csv", "dl_block_tests")
    else:
        st.info("Sheet `block_first_vs_rest_tests` not found.")

# ----------------------------
# TAB 6: Suspicious Blocks
# ----------------------------
with tab6:
    st.subheader("Suspicious blocks")
    if sus_blocks is not None:
        st.dataframe(sus_blocks, use_container_width=True)
        download_button_df(sus_blocks, "Download Suspicious Blocks (CSV)", "suspicious_blocks.csv", "dl_sus")
    else:
        st.info("Sheet `suspicious_blocks` not found.")
